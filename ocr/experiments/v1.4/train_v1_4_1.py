# train_v1.4.1.py - CRNN + CTC OCR í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° PyTorch ê´€ë ¨ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import wandb    # Weights & Biases: ì‹¤í—˜ ë¡œê¹… ë° ì‹œê°í™” ë„êµ¬
import random
import numpy as np
from torch.utils.data import DataLoader, WeightedRandomSampler
from collections import Counter
from torchvision import transforms

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from utils.dataset import OCRDataset, OCRLabelOnlyDataset   # ì´ë¯¸ì§€/ë ˆì´ë¸” ë°ì´í„°ì…‹ í´ë˜ìŠ¤
from utils.label_encoder import LabelEncoder    # í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”©/ë””ì½”ë”© í´ë˜ìŠ¤
from model.crnn import CRNN # CRNN ëª¨ë¸ ì •ì˜

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìˆ˜ì •
ocr_dir = os.path.dirname(os.path.abspath(__file__))
crnn_dir = os.path.abspath(os.path.join(ocr_dir, os.pardir))
bestmodel_dir = os.path.join(crnn_dir, "bestModel")
os.makedirs(bestmodel_dir, exist_ok=True)
best_path = os.path.join(bestmodel_dir, "ocr_best_model.pth")

# ---------------------------
# ë¬¸ì ë‹¨ìœ„ ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
# ---------------------------
def char_accuracy(gt, pred):
    """
    gt: ì‹¤ì œ í…ìŠ¤íŠ¸ (ground truth)
    pred: ì˜ˆì¸¡ í…ìŠ¤íŠ¸
    ë™ì¼í•œ ìœ„ì¹˜ì˜ ë¬¸ì ì¼ì¹˜ ë¹„ìœ¨ì„ ë°±ë¶„ìœ¨ë¡œ ë°˜í™˜
    """
    match = sum(g == p for g, p in zip(gt, pred))
    return match / max(len(gt), len(pred)) * 100

# ---------------------------
# Beam Search ë””ì½”ë”© í•¨ìˆ˜
# ---------------------------
def decode_beam_search(logits, encoder, beam_width=3):
    """
    logits: [W, B, C] - ì‹œê°„ì¶•, ë°°ì¹˜, í´ë˜ìŠ¤
    encoder: ì¸ì½”ë” ê°ì²´
    beam_width: top-k í›„ë³´ ìœ ì§€ ìˆ˜
    """
    log_probs = logits.log_softmax(2).cpu().detach().numpy()  # [W, B, C]
    log_probs = np.transpose(log_probs, (1, 0, 2))  # [B, W, C]

    results = []
    for seq in log_probs:
        paths = [([], 0)]  # ì´ˆê¸° ì‹œí€€ìŠ¤ì™€ ì ìˆ˜
        for t in seq:
            new_paths = []
            topk = np.argsort(t)[-beam_width:][::-1]  # í™•ë¥  ìƒìœ„ kê°œ ì„ íƒ
            for path, score in paths:
                for k in topk:
                    new_path = path + [k]
                    new_score = score + t[k]
                    new_paths.append((new_path, new_score))
            paths = sorted(new_paths, key=lambda x: x[1], reverse=True)[:beam_width]
        best_path = paths[0][0]
        decoded = encoder.decode_ctc_standard(best_path)
        results.append(decoded)
    return results

# ---------------------------
# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ (Beam Search ê¸°ë°˜)
# ---------------------------
def evaluate(model, val_loader, encoder, criterion):
    model.eval()
    total = 0
    correct = 0
    val_loss = 0.0

    with torch.no_grad():
        for images, texts in val_loader:
            images = images.to(device)
            outputs = model(images).log_softmax(2)

            # ë¼ë²¨ ì¸ì½”ë”©
            targets = [torch.tensor(encoder.encode(t), dtype=torch.long) for t in texts]
            target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)
            targets_concat = torch.cat(targets).to(device)
            input_lengths = torch.full(size=(images.size(0),), fill_value=outputs.size(0), dtype=torch.long).to(device)

            loss = criterion(outputs, targets_concat, input_lengths, target_lengths)
            val_loss += loss.item()

            # # Beam Search ë””ì½”ë”© ë° ì •í™•ë„ í‰ê°€
            decoded_texts = decode_beam_search(outputs, encoder, beam_width=5)
            for pred_text, gt_text in zip(decoded_texts, texts):
                total += 1
                if pred_text == gt_text:
                    correct += 1

    acc = correct / total * 100
    avg_loss = val_loss / total
    return acc, avg_loss

# ---------------------------
# ê· í˜•ì¡íŒ ìƒ˜í”ŒëŸ¬ ìƒì„± í•¨ìˆ˜ (ì ‘ë‘ì–´ ê¸°ë°˜)
# ---------------------------
def create_prefix_sampler(dataset, prefix_len=3):
    label_list = [dataset[i] for i in range(len(dataset))]  # ëª¨ë“  ë ˆì´ë¸” ì½ê¸°
    prefix_list = [label[:prefix_len] for label in label_list]  # ì ‘ë‘ì–´ ì¶”ì¶œ
    freq = Counter(prefix_list)
    weights = [1.0 / freq[label[:prefix_len]] for label in label_list]
    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)
    return sampler

# ---------------------------
# ë©”ì¸ í•™ìŠµ ë£¨í‹´
# ---------------------------
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # wandb ì´ˆê¸°í™”
    wandb.init(
        project="crnn-ocr",
        name="train_v1.4.1",
        dir="wandb_logs",
        config={
            "batch_size": 32,
            "lr": 0.0005,
            "epochs": 100,
            "hidden_size": 512,
            "decoder": "ctc_standard"
        }
    )
    config = wandb.config

    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜
    transform = transforms.Compose([
        transforms.Resize((32, 128)),
        transforms.RandomAffine(degrees=1, translate=(0.01, 0.01)),
        ## í•™ìŠµì‹œ '37ë°”'ì— ë¬¶ì—¬ë²„ë¦¬ëŠ” í˜„ìƒì˜ ì›ì¸ìœ¼ë¡œ íŒŒì•…ë¨
        #transforms.ColorJitter(brightness=0.2, contrast=0.2),
        #transforms.RandomPerspective(distortion_scale=0.2, p=0.5),
        #transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ ë° ë¼ë²¨ ì „ìš© ë°ì´í„°ì…‹ ë¡œë”©
    train_dataset = OCRDataset( image_dir='data/images/train', label_dir='data/labels/train', transform=transform )
    val_dataset = OCRDataset( image_dir='data/images/val', label_dir='data/labels/val', transform=transform )
    label_dataset = OCRLabelOnlyDataset( label_dir='data/labels/train' )
    
    # # ìƒ˜í”ŒëŸ¬ ì„¤ì • ë° ë°ì´í„°ë¡œë” ì •ì˜
    sampler = create_prefix_sampler(label_dataset, prefix_len=3)
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        sampler=sampler,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)

    # ì¸ì½”ë” ë° ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € êµ¬ì„±
    encoder = LabelEncoder()
    num_classes = encoder.num_classes()
    model = CRNN(num_classes=num_classes, hidden_size=config.hidden_size).to(device)
    criterion = nn.CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.Adam(model.parameters(), lr=config.lr)

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=config.lr,
        steps_per_epoch=len(train_loader),
        epochs=config.epochs,
        pct_start=0.1,
        anneal_strategy='cos',
        div_factor=25.0
    )

    best_acc = 0.0
    log = []
    pred_samples = []
    patience = 20  # ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì•„ë„ ê¸°ë‹¤ë¦´ ìµœëŒ€ epoch ìˆ˜
    counter = 0    # í˜„ì¬ patience ì¹´ìš´í„°

    # í•™ìŠµ ë£¨í”„
    try:
        for epoch in range(config.epochs):
            model.train()
            running_loss = 0.0
            for batch_idx, (images, texts) in enumerate(train_loader):
                images = images.to(device)

                targets = [torch.tensor(encoder.encode(t), dtype=torch.long) for t in texts]
                target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)
                targets_concat = torch.cat(targets).to(device)

                batch_size = images.size(0)
                output = model(images)
                input_lengths = torch.full(size=(batch_size,), fill_value=output.size(0), dtype=torch.long).to(device)

                loss = criterion(output.log_softmax(2), targets_concat, input_lengths, target_lengths)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                scheduler.step()

                running_loss += loss.item()

            # ê²€ì¦ í‰ê°€
            val_acc, val_loss = evaluate(model, val_loader, encoder, criterion)

            # ìƒ˜í”Œ ì˜ˆì¸¡ ë° ì •í™•ë„ ì¸¡ì •
            avg_char_acc = 0
            pred_samples.clear()
            with torch.no_grad():
                sample_indices = random.sample(range(len(train_dataset)), 10)
                for idx in sample_indices:
                    sample_img, sample_text = train_dataset[idx]
                    sample_img = sample_img.unsqueeze(0).to(device)
                    pred = model(sample_img)
                    pred_std = encoder.decode_ctc_standard(pred.softmax(2).argmax(2)[:, 0].cpu().numpy())
                    pred_beam = decode_beam_search(pred, encoder, beam_width=5)[0]
                    acc = char_accuracy(sample_text, pred_std)
                    avg_char_acc += acc
                    pred_samples.append((sample_text, pred_std, pred_beam, round(acc, 2)))
            avg_char_acc /= len(sample_indices)

            print(f"[Epoch {epoch+1}] Train Loss: {running_loss / len(train_loader):.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}% | Char Acc (í‰ê· ): {avg_char_acc:.4f}%")
            
            # ëª¨ë¸ ì €ì¥
            if not os.path.exists('checkpoints'):
                os.makedirs('checkpoints')
            torch.save(model.state_dict(), f'checkpoints/crnn_epoch_{epoch+1}.pth')
            print(f"âœ… ëª¨ë¸ ì €ì¥ë¨: checkpoints/crnn_epoch_{epoch+1}.pth")

            if val_acc > best_acc:
                best_acc = val_acc
                counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ë¦¬ì…‹
                torch.save(model.state_dict(), best_path)
                print(f"ğŸŒŸ í–¥ìƒ ëª¨ë¸ ê°±ì‹ ë¨! ì €ì¥ë¨: {best_path}")
            else:
                counter += 1
                print(f"â³ í–¥ìƒë˜ì§€ ì•ŠìŒ (counter: {counter}/{patience})")
                if counter >= patience:
                    print("â›”ï¸ Early stopping triggered!")
                    break

            # ì˜ˆì‹œ ë””ì½”ë”© ì¶œë ¥ ìœ ì§€
            for sample_text, pred_std, pred_beam, acc in pred_samples[:3]:
                print(f"[ìƒ˜í”Œ] GT: {sample_text}")
                print(f"  â”” Pred (í‘œì¤€ CTC): {pred_std} | Char Acc: {acc:.4f}%")
                print(f"  â”” Pred (Beam Search): {pred_beam}")

            # wandb ë¡œê¹…
            wandb.log({
                "epoch": epoch + 1,
                "loss": running_loss / len(train_loader),
                "val_loss": val_loss,
                "val_acc": val_acc,
                "char_accuracy": avg_char_acc,
                "lr": optimizer.param_groups[0]["lr"]
            })
            log.append((epoch + 1, running_loss / len(train_loader), val_loss, val_acc, avg_char_acc))

    except KeyboardInterrupt:
        print("\nâ›”ï¸ í•™ìŠµ ì¤‘ë‹¨ë¨! ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥ ì¤‘...")
        torch.save(model.state_dict(), 'checkpoints/last_interrupted.pth')
        print("ğŸ“‚ ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥ë¨: checkpoints/last_interrupted.pth")

    # CSVë¡œ í•™ìŠµ ë¡œê·¸ ì €ì¥
    wandb.finish()
    df = pd.DataFrame(log, columns=['epoch', 'train_loss', 'val_loss', 'val_acc', 'char_accuracy'])
    df.to_csv('train_logs/train_log.csv', index=False)
    print("ğŸ“„ í•™ìŠµ ë¡œê·¸ ì €ì¥ë¨: train_log.csv")